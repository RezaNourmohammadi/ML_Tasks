# Machine Learning and Sequence Processing Tasks

This repository contains the implementation of four distinct machine learning and sequence processing tasks. Each task demonstrates the application of various techniques to solve real-world problems. The following sections describe the problem statements and the implemented solutions.

## Task 1: Probability of Experiencing at Least n Rainy Days

**Objective:**  
The task involved determining the probability that it would rain on more than `n` days in a year, based on daily rain probabilities for each day of the year.

**Implementation:**  
- Data was loaded from a CSV file containing daily rain probabilities for each of the 365 days in a year. The data was cleaned and validated to ensure accuracy.
- Simulated rain data was generated by comparing random numbers to the day's rain probability to determine if it rained. This simulation was repeated for multiple samples to create a dataset representing possible yearly rain scenarios.
- A Random Forest Regressor was trained on this simulated dataset to learn the relationship between daily probabilities and the total number of rainy days.
- The model was validated to ensure its predictions were accurate.
- The trained model was then used to predict the probability of having at least `n` rainy days by comparing the predicted number of rainy days to the threshold `n`.

## Task 2: Phoneme Sequence to Words

**Objective:**  
The task involved finding all possible combinations of words from a pronunciation dictionary that could generate a given sequence of phonemes.

**Implementation:**  
- The pronunciation dictionary was loaded from a CSV file, and the phonemes for each word were split and encoded using `LabelEncoder`.
- Phonemes were padded to ensure a consistent input size for the model.
- A sequence-to-sequence model with Long Short-Term Memory (LSTM) layers was used to map the phoneme sequences to words. The encoder processed the phoneme sequence, and the decoder generated the corresponding word sequence.
- After training, the model was used to predict the word sequence for an input phoneme sequence by encoding it, passing it through the model, and generating the most probable word sequence.

## Task 3: Finding the n Most Frequent Words in the TensorFlow Shakespeare Dataset

**Objective:**  
The task was to find the `n` most frequent words in the TensorFlow Shakespeare dataset.

**Implementation:**  
- Two implementations were considered: one in C and one in Python.
- **C Implementation:** A `WordFreq` structure was used to store each word and its frequency. Words were read from the dataset, converted to lowercase, and their frequencies were updated. The words were then sorted by frequency using the `qsort` function.
- **Python Implementation:** The Python solution leveraged built-in libraries to read the entire text file, convert it to lowercase, and use the `Counter` class from the `collections` module to count word frequencies. The `most_common` method of `Counter` was used to retrieve the `n` most frequent words.
- The Python solution was found to be more efficient and concise due to Pythonâ€™s high-level data structures and libraries.

## Task 4: Implementing CTC as Described in the Paper

**Objective:**  
The task was to implement Connectionist Temporal Classification (CTC), a method for sequence-to-sequence problems, commonly used in speech recognition and handwriting recognition.

**Implementation:**  
- A PyTorch-based solution was developed with a custom dataset class to handle data loading and preprocessing.
- The data was converted into tensors suitable for model training.
- A collation function was implemented to handle batch processing, ensuring sequences within each batch were padded to a uniform length.
- The function also constructed tensors for input and target lengths, which are essential for calculating CTC loss.
- A PyTorch DataLoader was used to handle batching, ensuring the data was correctly formatted for training the CTC model.

## Conclusion

In this repository, we addressed several machine learning and sequence processing tasks that involved predicting probabilities, text processing, and sequence alignment. Here is a summary of each task:

1. **Rain Prediction:** We implemented a Random Forest Regressor to predict the probability of having at least `n` rainy days based on daily probabilities.
2. **Phoneme to Word Mapping:** We built a sequence-to-sequence model using LSTM layers to map phoneme sequences to words.
3. **Word Frequency Analysis:** We compared C and Python implementations for finding the most frequent words in the TensorFlow Shakespeare dataset, with Python proving to be more efficient.
4. **CTC Implementation:** We developed a PyTorch-based implementation for Connectionist Temporal Classification, used in sequence-to-sequence learning tasks like speech recognition.

These implementations demonstrate the versatility and power of modern machine learning frameworks for solving complex real-world problems. Each task highlights the application of different techniques, such as supervised learning, sequence processing, and custom neural network models, which are essential for a wide range of applications.
